{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download housing data\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "        \n",
    "fetch_housing_data(HOUSING_URL, HOUSING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the data to a more readable pandas format\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "housing = load_housing_data()\n",
    "print(housing.head(10))\n",
    "#housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only in a Jupyter notebook\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that splits the training set\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def test_set_check(identifier, test_ratio, hash):\n",
    "    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training set\n",
    "\n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "# below is the easier way to do it with just column id's, however adding or taking out rows will change the test set\n",
    "# housing_with_id = housing.reset_index() # adds an `index` column\n",
    "# train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset and test set with Scikit-Learn. Better if you have multiple datasets, also simpler.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the test set with stratified dataset\n",
    "\n",
    "# The following code creates\n",
    "# an income category attribute by dividing the median income by 1.5 (to limit the number of income\n",
    "# categories), and rounding up using ceil (to have discrete categories), and then merging all the categories\n",
    "# greater than 5 into category 5\n",
    "\n",
    "\n",
    "#create startum (catagories)\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "#import\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "#split training set with startum in mind\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "    \n",
    "    \n",
    "#Now remove the income_cat attribute so the data is back to its original state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "s=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ") \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'median_house_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-51a0e6bf2eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorr_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorr_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"median_house_value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'median_house_value'"
     ]
    }
   ],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array(['cat','amsterdam','boy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "\n",
    "import os, tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from six.moves import urllib\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#Download housing data\n",
    "def fetch_housing_data(housing_url, housing_path):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "\n",
    "#convert the data to a more readable pandas format\n",
    "def load_housing_data(housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "#stratum split\n",
    "def stratum_split(housing):\n",
    "\t# The following code creates\n",
    "\t# an income category attribute by dividing the median income by 1.5 (to limit the number of income\n",
    "\t# categories), and rounding up using ceil (to have discrete categories), and then merging all the categories\n",
    "\t# greater than 5 into category 5\n",
    "\n",
    "\n",
    "\t#create startum (catagories)\n",
    "\thousing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "\thousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "\t#split training set with startum in mind\n",
    "\tsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\tfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "\t    strat_train_set = housing.loc[train_index]\n",
    "\t    strat_test_set = housing.loc[test_index]\n",
    "\t    \n",
    "\t#Now remove the income_cat attribute so the data is back to its original state\n",
    "\tfor set_ in (strat_train_set, strat_test_set):\n",
    "\t    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "\treturn strat_train_set, strat_test_set\n",
    "\n",
    "def interpret_data(housing):\n",
    "\thousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "\thousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "\ts=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "\tc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "\t) \n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "def main():\n",
    "\tDOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "\tHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "\tHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "\tfetch_housing_data(HOUSING_URL, HOUSING_PATH) #load data\n",
    "\n",
    "\thousing = load_housing_data(HOUSING_PATH) # convert to pandas dataframe\n",
    "\t\n",
    "\thousing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "\thousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "\thousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\t# split test set with scikit-learn, instead we use the more acurate stratum split\n",
    "\t#train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) \n",
    "\n",
    "\ttrain_set, test_set = stratum_split(housing)\n",
    "\tprint(len(train_set))\n",
    "\tprint(len(test_set))\n",
    "\n",
    "\t# interpret_data(housing)\n",
    "\n",
    "main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# # --------------------- splitting the test set with stratified dataset ------------------------------------------\n",
    "\n",
    "\t# # The following code creates\n",
    "\t# # an income category attribute by dividing the median income by 1.5 (to limit the number of income\n",
    "\t# # categories), and rounding up using ceil (to have discrete categories), and then merging all the categories\n",
    "\t# # greater than 5 into category 5\n",
    "\n",
    "\n",
    "\t# #create startum (catagories)\n",
    "\t# housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "\t# housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "\t# #split training set with startum in mind\n",
    "\t# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\t# for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "\t#     strat_train_set = housing.loc[train_index]\n",
    "\t#     strat_test_set = housing.loc[test_index]\n",
    "\t    \n",
    "\t# #Now remove the income_cat attribute so the data is back to its original state\n",
    "\t# for set_ in (strat_train_set, strat_test_set):\n",
    "\t#     set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "\t# # -------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "fetch_housing_data(HOUSING_URL, HOUSING_PATH) #load data\n",
    "\n",
    "housing = load_housing_data(HOUSING_PATH) # convert to pandas dataframe\n",
    "\n",
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "# split test set with scikit-learn, instead we use the more acurate stratum split\n",
    "#train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) \n",
    "\n",
    "train_set, test_set = stratum_split(housing)\n",
    "print(len(train_set))\n",
    "print(len(test_set))\n",
    "\n",
    "interpret_data(housing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_data(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling in missing values with imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num)\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns)\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#secifically fill in the missing columns for the total bedrooms. NOT A GENERAL SOLUTION\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median() # option 3\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text features\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded\n",
    "ocean_labels = encoder.classes_\n",
    "print(ocean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarize a text column into many \n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer(sparse_output=True) # make sparse_ouput=False for a dense NumPy Array. Boooooooo!\n",
    "housing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot\n",
    "\n",
    "ocean_labels = encoder.classes_ # just to use for reference to know what is what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
      "(16512, 9)\n",
      "DataFrameSelector:\n",
      "(16512, 8)\n",
      "combined attributes:\n",
      "(16512, 11)\n",
      "DataFrameSelector:\n",
      "(16512, 1)\n",
      "(16512, 16)\n",
      "(16512, 16)\n"
     ]
    }
   ],
   "source": [
    "#Main\n",
    "\n",
    "import os, tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from six.moves import urllib\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "#Download housing data\n",
    "def fetch_housing_data(housing_url, housing_path):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "\n",
    "#convert the data to a more readable pandas format\n",
    "def load_housing_data(housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "#stratum split\n",
    "def stratum_split(housing):\n",
    "\t# The following code creates\n",
    "\t# an income category attribute by dividing the median income by 1.5 (to limit the number of income\n",
    "\t# categories), and rounding up using ceil (to have discrete categories), and then merging all the categories\n",
    "\t# greater than 5 into category 5\n",
    "\n",
    "\n",
    "\t#create startum (catagories)\n",
    "\thousing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "\thousing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "\t#split training set with stratum in mind\n",
    "\tsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\tfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "\t    strat_train_set = housing.loc[train_index]\n",
    "\t    strat_test_set = housing.loc[test_index]\n",
    "\t    \n",
    "\t#Now remove the income_cat attribute so the data is back to its original state\n",
    "\tfor set_ in (strat_train_set, strat_test_set, housing):\n",
    "\t    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "\treturn strat_train_set, strat_test_set\n",
    "\n",
    "def interpret_data(housing):\n",
    "\thousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "\thousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "\ts=housing[\"population\"]/100, label=\"population\", figsize=(10,7),\n",
    "\tc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "\t) \n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "# #Binarize a text column into many \n",
    "# # from sklearn.preprocessing import LabelBinarizer\n",
    "# def binarize_ocean_proximity(housing):\n",
    "\n",
    "# \tencoder = LabelBinarizer(sparse_output=True) # make sparse_ouput=False for a dense NumPy Array. Boooooooo!\n",
    "# \thousing_cat_1hot = encoder.fit_transform(housing_cat)\n",
    "# \thousing_cat_1hot\n",
    "# \tocean_labels = encoder.classes_ # just to use for reference to know what is what.\n",
    "\n",
    "class LabelBinarizerPipelineFriendly(LabelBinarizer):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n",
    "        super(LabelBinarizerPipelineFriendly, self).fit(X)\n",
    "    def transform(self, X, y=None):\n",
    "        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)\n",
    "\n",
    "#Custom transformer class to  add new attributes to house data\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "\tdef __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "\t\tself.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self # nothing else to do\n",
    "\tdef transform(self, X, y=None):\n",
    "\t\trooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\t\trooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "\t\tpopulation_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "\t\tif self.add_bedrooms_per_room:\n",
    "\t\t\tbedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "\t\t\tprint(\"combined attributes:\")\n",
    "\t\t\tprint(np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room].shape)\n",
    "\t\t\treturn np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "\t\telse:\n",
    "\t\t\treturn np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "# attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "# housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "\tdef __init__(self, attribute_names):\n",
    "\t\tself.attribute_names = attribute_names\n",
    "\tdef fit(self, X, y=None):\n",
    "\t\treturn self\n",
    "\tdef transform(self, X):\n",
    "\t\tprint(\"DataFrameSelector:\")\n",
    "\t\tprint(X[self.attribute_names].values.shape)\n",
    "\t\treturn X[self.attribute_names].values\n",
    "\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "fetch_housing_data(HOUSING_URL, HOUSING_PATH) #load data\n",
    "\n",
    "housing = load_housing_data(HOUSING_PATH) # convert to pandas dataframe\n",
    "\n",
    "\n",
    "#housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "#housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "#housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "\n",
    "# split test set with scikit-learn, instead we use the more acurate stratum split\n",
    "#train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42) \n",
    "\n",
    "train_set, test_set = stratum_split(housing)\n",
    "\n",
    "housing = train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = train_set[\"median_house_value\"].copy()\n",
    "\n",
    "#interpret_data(housing)\n",
    "training_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(training_num)\n",
    "print(num_attribs)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "# \thousing_num_tr = num_pipeline.fit_transform(training_num)\n",
    "# \tprint(housing_num_tr.shape)\n",
    "cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('label_binarizer', LabelBinarizerPipelineFriendly()),\n",
    "    ])\n",
    "# \tcat = cat_pipeline.fit_transform(train_set)\n",
    "# \tprint(cat.shape)\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])\n",
    "print(housing.shape)\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "#print(housing_prepared)\n",
    "print(housing_prepared.shape)\n",
    "print(housing_prepared.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 9)\n",
      "DataFrameSelector:\n",
      "(5, 8)\n",
      "combined attributes:\n",
      "(5, 11)\n",
      "DataFrameSelector:\n",
      "(5, 1)\n",
      "(5, 16)\n",
      "('Predictions:', array([210644.60459286, 317768.80697211, 210956.43331178,  59218.98886849,\n",
      "       189747.55849879]))\n",
      "('Labels:', [286600.0, 340600.0, 196900.0, 46300.0, 254500.0])\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "print(some_data.shape)\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(some_data_prepared.shape)\n",
    "print(\"Predictions:\", lin_reg.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68628.19819848923\n"
     ]
    }
   ],
   "source": [
    "#check mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def check_mse(data, labels, model):\n",
    "    \n",
    "    predictions = model.predict(data)\n",
    "    model_mse = mean_squared_error(labels, predictions)\n",
    "    model_rmse = np.sqrt(model_mse)\n",
    "    print(model_rmse)\n",
    "\n",
    "check_mse(housing_prepared, housing_labels, lin_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def check_mse_cross_validation(data, labels, model):\n",
    "    scores = cross_val_score(model, data, labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    tree_rmse_scores = np.sqrt(-scores)\n",
    "    return tree_rmse_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make decision tree model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#check mse for decission tree\n",
    "check_mse(housing_prepared, housing_labels, tree_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Scores:', array([68412.16768296, 67320.32091185, 69136.82824501, 67778.09585112,\n",
      "       71017.80203566, 75446.66825312, 70404.31147174, 71274.36683137,\n",
      "       77502.15531233, 69932.6442633 ]))\n",
      "('Mean:', 70822.53608584664)\n",
      "('Standard deviation:', 3120.811186835279)\n"
     ]
    }
   ],
   "source": [
    "tree_rmse_scores = check_mse_cross_validation(housing_prepared, housing_labels, tree_reg)\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Scores:', array([51761.59314537, 49519.06351825, 52218.51312455, 54695.67527694,\n",
      "       52698.91723318, 55406.90596006, 51207.08033053, 50881.01775344,\n",
      "       55517.98337045, 54048.16607546]))\n",
      "('Mean:', 52795.49157882074)\n",
      "('Standard deviation:', 1942.826702940512)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "forest_rmse_scores = check_mse_cross_validation(housing_prepared, housing_labels, forest_reg)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 8, 'n_estimators': 30}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the best hyperparameter fit --- we should look up more information on this\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameSelector:\n",
      "(4128, 8)\n",
      "combined attributes:\n",
      "(4128, 11)\n",
      "DataFrameSelector:\n",
      "(4128, 1)\n"
     ]
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) # => evaluates to 47,766.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47726.76421578336\n"
     ]
    }
   ],
   "source": [
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
